### Four 'articles of faith' that may cause serious problems
1. Overrating accuracy if we ignore false positives
2. Replacement of causation with correlation
3. The idea that sampling bias is eliminated when the issues still remain
4. Letting the 'data speak' and ignoring the presence of spurious correlations

### Two different approaches towards analysis
1. Statistical or data models: A researcher specified model that is then estimated using the data available to assess model fit and ultimately its acceptability.
- Challenges: correctly specify a model form that represents the process being examined, and perform the analysis correctly to provide the explanation and predition desired.
- The researcher must take some assumptions as to the nature of the process and specify a model form that best replicates its operations.
- Examples: MANOVA, logistic regressions, etc.
- The researcher must recognize that any conclusions are about the proposed model and not really about the underlying process.
- Usually for explanation, is theory based, structured, type of data analyzed are well defined, collected for purpose of the research.

2. Data mining or algorithm models: The focus is not on the specified model, but the technique of explanation.
- The fundamental premise here is that the process being studied is inherently so complex that specification of a precise model is impossible.
- Rather, the emphasis is on the algorithms, how they can represent any complex process and how well they ultimately predict the outcomes.
- Examples: neutrual networks, decision trees and even cluster analysis - where the result is a prediction, not a generalization to the population.
- Usually for prediction, heuristic-based, unstructured, exploratory, undefined, generally analysis using data available.

##Causal inference





